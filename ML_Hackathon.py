# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GMsZmq90ftkn-XLx2qzkcQtPcKmfBG3Z
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os, random, numpy as np, torch, torch.nn as nn, torch.optim as optim
import matplotlib.pyplot as plt

# Simple internal word list
WORDS = [
    "apple", "banana", "grape", "orange", "lemon", "mango", "cherry",
    "papaya", "peach", "plum", "guava", "melon", "berry", "dates"
]
print(f"Loaded {len(WORDS)} words.")

MAX_WRONG = 6
MAX_WORD_LEN = 10  # fix to max length of words in WORDS

class HangmanEnv:
    def __init__(self, words):
        self.words = [w for w in words if len(w) <= MAX_WORD_LEN]
        self.max_wrong = MAX_WRONG
        self.reset()

    def reset(self):
        self.word = random.choice(self.words)
        self.mask = "_" * len(self.word)
        self.guessed = set()
        self.wrong = 0
        self.done = False
        return self.get_state()

    def get_state(self):
        guessed_vec = np.array([1 if chr(97+i) in self.guessed else 0 for i in range(26)])
        mask_vec = np.zeros((MAX_WORD_LEN, 26))
        for i, c in enumerate(self.mask):
            if c != "_":
                mask_vec[i, ord(c)-97] = 1
        mask_flat = mask_vec.flatten()
        lives = np.array([self.max_wrong - self.wrong])
        return np.concatenate([guessed_vec, mask_flat, lives])

    def step(self, action):
        letter = chr(97 + action)
        reward = 0
        if letter in self.guessed:
            reward -= 0.5
        else:
            self.guessed.add(letter)
            if letter in self.word:
                new_mask = "".join([letter if self.word[i] == letter else self.mask[i] for i in range(len(self.word))])
                reward += 2 * new_mask.count(letter)
                self.mask = new_mask
                if "_" not in self.mask:
                    self.done = True
                    reward += 10
            else:
                self.wrong += 1
                reward -= 1
                if self.wrong >= self.max_wrong:
                    self.done = True
                    reward -= 5
        return self.get_state(), reward, self.done

class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    def forward(self, x):
        return self.layers(x)

env = HangmanEnv(WORDS)
state_dim = len(env.reset())
action_dim = 26

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

policy_net = DQN(state_dim, action_dim).to(device)
target_net = DQN(state_dim, action_dim).to(device)
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()

optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)
loss_fn = nn.MSELoss()

print("State dim:", state_dim)

episodes = 1000
gamma = 0.9
eps = 0.1
batch_size = 32
replay = []
reward_history = []

for ep in range(episodes):
    s = env.reset()
    done = False
    total_reward = 0

    while not done:
        if random.random() < eps:
            a = random.randint(0, 25)
        else:
            a = policy_net(torch.tensor(s, dtype=torch.float32).to(device)).argmax().item()

        s2, r, done = env.step(a)
        replay.append((s, a, r, s2, done))
        s = s2
        total_reward += r

        if len(replay) > batch_size:
            batch = random.sample(replay, batch_size)
            s_b = torch.tensor([x[0] for x in batch], dtype=torch.float32).to(device)
            a_b = torch.tensor([x[1] for x in batch], dtype=torch.int64).unsqueeze(1).to(device)
            r_b = torch.tensor([x[2] for x in batch], dtype=torch.float32).unsqueeze(1).to(device)
            s2_b = torch.tensor([x[3] for x in batch], dtype=torch.float32).to(device)
            d_b = torch.tensor([x[4] for x in batch], dtype=torch.float32).unsqueeze(1).to(device)

            q_val = policy_net(s_b).gather(1, a_b)
            q_next = target_net(s2_b).max(1)[0].detach().unsqueeze(1)
            q_target = r_b + gamma * q_next * (1 - d_b)

            loss = loss_fn(q_val, q_target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    reward_history.append(total_reward)
    if ep % 500 == 0:
        print(f"Episode {ep}/{episodes}, Avg Reward: {np.mean(reward_history[-500:]):.2f}")

print("âœ… Training Finished")

def evaluate(model, n_games=2000):
    model.eval()
    total_wins, total_wrong, total_reward = 0, 0, 0

    for _ in range(n_games):
        s = env.reset()
        done = False
        while not done:
            a = model(torch.tensor(s, dtype=torch.float32).to(device)).argmax().item()
            s, r, done = env.step(a)
            total_reward += r
            if done and "_" not in env.mask:
                total_wins += 1
            else:
                total_wrong += env.wrong
    acc = (total_wins / n_games) * 100
    print(f"\nðŸŽ¯ TEST RESULTS ðŸŽ¯")
    print(f"Wins: {total_wins}/{n_games}  |  Accuracy: {acc:.2f}%")
    print(f"Avg Reward: {total_reward/n_games:.2f}")
    return acc

evaluate(policy_net)

plt.plot(reward_history)
plt.title("Training Rewards Over Time")
plt.xlabel("Episodes")
plt.ylabel("Total Reward")
plt.show()



